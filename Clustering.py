import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from geopy.distance import great_circle
from shapely.geometry import MultiPoint
import folium
pd.options.display.max_columns = None

# ë¬¸í™” ì‹œì„¤
ulsan_mun = pd.read_csv('C:/Users/data/ulsan/ULSAN_MUN_DATA.csv',encoding='euc-kr')
ulsan_mun.info()

ulsan_mun_f = ulsan_mun[['ì•„ì´ë””(ID)', 'ì—¬í–‰ì§€ëª…', 'ëŒ€êµ¬ë¶„', 'ì¤‘êµ¬ë¶„', 'ì†Œêµ¬ë¶„', 'ìœ„ë„ê°’', 'ê²½ë„ê°’']].copy()
ulsan_mun_f = ulsan_mun_f.drop_duplicates()
ulsan_mun_r = ulsan_mun_f.rename(columns={'ì•„ì´ë””(ID)':'ID', 'ì—¬í–‰ì§€ëª…':'TRANM', 'ëŒ€êµ¬ë¶„':'SEG1', 'ì¤‘êµ¬ë¶„':'SEG2', 'ì†Œêµ¬ë¶„':'SEG3', 'ìœ„ë„ê°’':'LATITUDE', 'ê²½ë„ê°’':'LONGITUDE'})

# ì—¬í–‰ì§€ëª…
ulsan_tra = pd.read_csv('C:/Users/data/ulsan/ULSAN_TRA_DATA.csv',encoding='euc-kr')
ulsan_tra.info()

ulsan_tra_f = ulsan_tra[['ì•„ì´ë””(ID)','ì—¬í–‰ì§€ëª…', 'ëŒ€êµ¬ë¶„', 'ì¤‘êµ¬ë¶„', 'ì†Œêµ¬ë¶„', 'ìœ„ë„ê°’', 'ê²½ë„ê°’']].copy()
ulsan_tra_f = ulsan_tra_f.drop_duplicates()
ulsan_tra_r = ulsan_tra_f.rename(columns={'ì•„ì´ë””(ID)':'ID','ì—¬í–‰ì§€ëª…':'TRANM', 'ëŒ€êµ¬ë¶„':'SEG1', 'ì¤‘êµ¬ë¶„':'SEG2', 'ì†Œêµ¬ë¶„':'SEG3', 'ìœ„ë„ê°’':'LATITUDE', 'ê²½ë„ê°’':'LONGITUDE'})

# ë¬¸í™”ì‹œì„¤ + ì—¬í–‰ì§€ëª…
ulsan_data = pd.concat([ulsan_mun_r, ulsan_tra_r], axis = 0).reset_index(drop = True)
ulsan_data = ulsan_data.drop_duplicates()

# í•¨ê»˜ê²€ìƒ‰ ê±´ìˆ˜ : ê´€ê´‘ì§€ ì¤‘ìš”ë„
ulsan_cnt = pd.read_csv('C:/Users/data/ulsan/ULSAN_TO_COUNT.csv',encoding='cp949')
ulsan_cnt.info()

# ê´€ê´‘ì§€ ì¤‘ìš”ë„ merge
ulsan_data1 = pd.merge(ulsan_data, ulsan_cnt, on = 'ID', how = 'left')
ulsan_data1.info()



# êµ°ì§‘ë¶„ì„ Start

import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from shapely.geometry import MultiPoint, Point, LineString
import networkx as nx
from geopy.distance import great_circle
import itertools
from scipy.spatial.distance import pdist
import geopandas as gpd

# =====================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# =====================================
# ulsan_data1 = pd.read_csv("your_data.csv")  # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
# í•„ìš”í•œ ì»¬ëŸ¼: LATITUDE, LONGITUDE, COUNT (ì´ìš©ê±´ìˆ˜)

# =====================================
# 2. DBSCAN êµ°ì§‘ë¶„ì„ (ê°€ì¤‘ì¹˜ ë°˜ì˜)
# =====================================

# ì¢Œí‘œ ë³€í™˜
coords = ulsan_data1[['LATITUDE', 'LONGITUDE']].values

# eps ê³„ì‚° (2km ë°˜ê²½)
kms_per_radian = 6371.0088
eps = 2 / kms_per_radian  # 2km ê¸°ì¤€

# ì´ìš©ê±´ìˆ˜ë¥¼ 0~1ë¡œ ì •ê·œí™”
ulsan_data1['Value'] = ulsan_data1['COUNT'] / ulsan_data1['COUNT'].max()

# ì¸ê¸° ê´€ê´‘ì§€ì˜ ì˜í–¥ë ¥ì„ ì¢Œí‘œ ê°€ì¤‘ì¹˜ë¡œ ë°˜ì˜
weighted_coords = np.column_stack([
    ulsan_data1['LATITUDE'] + (ulsan_data1['Value'] - 0.5) * 0.002,
    ulsan_data1['LONGITUDE'] + (ulsan_data1['Value'] - 0.5) * 0.002
])

# DBSCAN ì‹¤í–‰
db = DBSCAN(eps=eps, min_samples=3, algorithm='ball_tree', metric='haversine').fit(np.radians(weighted_coords))
cluster_labels = db.labels_

ulsan_data1['cluster'] = cluster_labels

print(f"ì´ êµ°ì§‘ ìˆ˜: {len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)}")
print(f"ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ìˆ˜: {sum(cluster_labels == -1)}")

# =====================================
# 3. êµ°ì§‘ë³„ í†µê³„ ë° í’ˆì§ˆ í‰ê°€
# =====================================

clusters = []
for cluster_id in set(cluster_labels):
    if cluster_id == -1:
        continue  # ë…¸ì´ì¦ˆ ì œì™¸

    cluster_data = ulsan_data1[ulsan_data1['cluster'] == cluster_id]
    cluster_points = cluster_data[['LATITUDE', 'LONGITUDE']].values
    centroid = MultiPoint(cluster_points).centroid

    # êµ°ì§‘ ì‘ì§‘ë„ ê³„ì‚° (êµ°ì§‘ ë‚´ í‰ê·  ê±°ë¦¬)
    compactness = 0.0
    if len(cluster_points) > 1:
        # haversine ì§ì ‘ ê³„ì‚°
        from itertools import combinations

        distances = []
        for (lat1, lon1), (lat2, lon2) in combinations(cluster_points, 2):
            dist = great_circle((lat1, lon1), (lat2, lon2)).km
            distances.append(dist)
        compactness = np.mean(distances) if distances else 0.0

    clusters.append({
        'cluster_id': cluster_id,
        'center_lat': centroid.y,
        'center_lon': centroid.x,
        'tour_spot_count': len(cluster_data),
        'avg_usage': cluster_data['COUNT'].mean(),
        'total_usage': cluster_data['COUNT'].sum(),
        'compactness_km': round(compactness, 2)
    })

cluster_summary = pd.DataFrame(clusters)
cluster_summary = cluster_summary.sort_values('total_usage', ascending=False).reset_index(drop=True)

print(cluster_summary.to_string(index=False))

# =====================================
# 4. ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± (ìˆ˜ìš” ê¸°ë°˜ ê°€ì¤‘ì¹˜)
# =====================================
# ìœ„ë„/ê²½ë„ê°€ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸ (ìš¸ì‚°ì€ ìœ„ë„ 35Â°, ê²½ë„ 129Â° ë¶€ê·¼)
if cluster_summary['center_lat'].mean() > 90 or cluster_summary['center_lat'].mean() < -90:
    print("\nâš ï¸  WARNING: Latitude/Longitude ì»¬ëŸ¼ì´ ë°”ë€ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ìë™ êµì •í•©ë‹ˆë‹¤.")
    cluster_summary.rename(columns={'center_lat': 'center_lon', 'center_lon': 'center_lat'}, inplace=True)
    print(f"êµì • í›„ Latitude ë²”ìœ„: {cluster_summary['center_lat'].min():.6f} ~ {cluster_summary['center_lat'].max():.6f}")
    print(f"êµì • í›„ Longitude ë²”ìœ„: {cluster_summary['center_lon'].min():.6f} ~ {cluster_summary['center_lon'].max():.6f}")

# êµ°ì§‘ ê°„ ê±°ë¦¬ ê³„ì‚°
edges = []
for (i, j) in itertools.combinations(range(len(cluster_summary)), 2):
    point_i = (cluster_summary.iloc[i]['center_lat'], cluster_summary.iloc[i]['center_lon'])
    point_j = (cluster_summary.iloc[j]['center_lat'], cluster_summary.iloc[j]['center_lon'])
    dist = great_circle(point_i, point_j).km
    edges.append((cluster_summary.iloc[i]['cluster_id'],
                  cluster_summary.iloc[j]['cluster_id'],
                  dist))

# ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
G = nx.Graph()
for _, row in cluster_summary.iterrows():
    G.add_node(
        row['cluster_id'],
        pos=(row['center_lon'], row['center_lat']),
        weight=row['total_usage']
    )

# ìˆ˜ìš” ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¡œ ì—£ì§€ ì¶”ê°€
for a, b, dist in edges:
    weight_a = cluster_summary[cluster_summary['cluster_id'] == a]['total_usage'].values[0]
    weight_b = cluster_summary[cluster_summary['cluster_id'] == b]['total_usage'].values[0]

    # ê°€ì¤‘ì¹˜: ê±°ë¦¬ / (ë‘ êµ°ì§‘ ìˆ˜ìš”ì˜ í•©)
    # â†’ ìˆ˜ìš”ê°€ ë†’ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ê°ì†Œ = ìš°ì„  ì—°ê²°
    adjusted_weight = dist / (weight_a + weight_b + 1)

    G.add_edge(a, b, weight=adjusted_weight, distance=dist)

print(f"ë…¸ë“œ ìˆ˜: {G.number_of_nodes()}")
print(f"ì—£ì§€ ìˆ˜: {G.number_of_edges()}")

# =====================================
# 5. MST ê¸°ë°˜ ê¸°ë³¸ ë…¸ì„  ë„¤íŠ¸ì›Œí¬
# =====================================
print("\n" + "=" * 50)
print("STEP 4: MST ê¸°ë°˜ ê¸°ë³¸ ë…¸ì„  ìƒì„±")
print("=" * 50)

mst = nx.minimum_spanning_tree(G, weight='weight')

routes_mst = []
for u, v, data in mst.edges(data=True):
    actual_distance = G[u][v]['distance']
    routes_mst.append({
        'start_cluster': u,
        'end_cluster': v,
        'distance_km': round(actual_distance, 2),
        'adjusted_weight': round(data['weight'], 4)
    })

route_mst_df = pd.DataFrame(routes_mst)

# ìš°ì„ ìˆœìœ„ í‘œì‹œ (ìƒìœ„ 3ê°œ êµ°ì§‘ í¬í•¨ ë…¸ì„ )
top_clusters = cluster_summary.head(3)['cluster_id'].tolist()
route_mst_df['priority'] = route_mst_df.apply(
    lambda x: 1 if (x['start_cluster'] in top_clusters or x['end_cluster'] in top_clusters) else 0,
    axis=1
)

print("\n[MST ê¸°ë°˜ ë…¸ì„ ]")
print(route_mst_df.to_string(index=False))

# =====================================
# 6. TSP ê¸°ë°˜ ìˆœí™˜ ë…¸ì„  (ìƒìœ„ Nê°œ êµ°ì§‘)
# =====================================
print("\n" + "=" * 50)
print("STEP 5: TSP ê¸°ë°˜ ìˆœí™˜ ë…¸ì„  ìƒì„±")
print("=" * 50)

# ìƒìœ„ 5ê°œ ì¸ê¸° êµ°ì§‘ ì„ íƒ
top_n = min(5, len(cluster_summary))
top_n_clusters = cluster_summary.head(top_n)['cluster_id'].tolist()

print(f"ìˆœí™˜ ë…¸ì„  ëŒ€ìƒ êµ°ì§‘: {top_n_clusters}")

# ë¶€ë¶„ ê·¸ë˜í”„ ìƒì„±
subgraph = G.subgraph(top_n_clusters)

try:
    # Greedy TSP ê·¼ì‚¬
    tsp_path = nx.approximation.greedy_tsp(subgraph, weight='weight', source=top_n_clusters[0])

    # ê²½ë¡œë¥¼ ìˆœì„œëŒ€ë¡œ ì €ì¥
    tsp_routes = []
    total_distance = 0

    for i in range(len(tsp_path) - 1):
        u, v = tsp_path[i], tsp_path[i + 1]
        dist = G[u][v]['distance']
        total_distance += dist

        tsp_routes.append({
            'sequence': i + 1,
            'from_cluster': u,
            'to_cluster': v,
            'distance_km': round(dist, 2)
        })

    tsp_route_df = pd.DataFrame(tsp_routes)

    print("\n[TSP ìˆœí™˜ ë…¸ì„ ]")
    print(tsp_route_df.to_string(index=False))
    print(f"\nì´ ìˆœí™˜ ê±°ë¦¬: {round(total_distance, 2)} km")

except Exception as e:
    print(f"TSP ê²½ë¡œ ìƒì„± ì‹¤íŒ¨: {e}")
    print("MST ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    tsp_route_df = None

# =====================================
# 7. CSV ê²°ê³¼ ì €ì¥
# =====================================

output_path = "C:/Users/data/ulsan/"

# ì›ë³¸ ë°ì´í„° (êµ°ì§‘ ì •ë³´ í¬í•¨)
ulsan_data1.to_csv(f"{output_path}ulsan_tour_clusters.csv", index=False, encoding='utf-8-sig')
print(f"âœ“ {output_path}ulsan_tour_clusters.csv")

# êµ°ì§‘ ìš”ì•½
cluster_summary.to_csv(f"{output_path}ulsan_cluster_summary.csv", index=False, encoding='utf-8-sig')
print(f"âœ“ {output_path}ulsan_cluster_summary.csv")

# MST ë…¸ì„ 
route_mst_df.to_csv(f"{output_path}ulsan_yegaro_mst_routes.csv", index=False, encoding='utf-8-sig')
print(f"âœ“ {output_path}ulsan_yegaro_mst_routes.csv")

# TSP ìˆœí™˜ ë…¸ì„ 
if tsp_route_df is not None:
    tsp_route_df.to_csv(f"{output_path}ulsan_yegaro_tsp_route.csv", index=False, encoding='utf-8-sig')
    print(f"âœ“ {output_path}ulsan_yegaro_tsp_route.csv")

# =====================================
# 8. QGIS ì‹œê°í™”ìš© GeoJSON ìƒì„±
# =====================================
print("\n" + "=" * 50)
print("STEP 7: GeoJSON ìƒì„± (QGISìš©)")
print("=" * 50)

# êµ°ì§‘ ì¤‘ì‹¬ì  GeoDataFrame
gdf_clusters = gpd.GeoDataFrame(
    cluster_summary,
    geometry=[Point(row['center_lon'], row['center_lat']) for _, row in cluster_summary.iterrows()],
    crs='EPSG:4326'
)

# MST ë…¸ì„  LineString
mst_geometries = []
for _, route in route_mst_df.iterrows():
    start = cluster_summary[cluster_summary['cluster_id'] == route['start_cluster']].iloc[0]
    end = cluster_summary[cluster_summary['cluster_id'] == route['end_cluster']].iloc[0]
    line = LineString([
        (start['center_lon'], start['center_lat']),
        (end['center_lon'], end['center_lat'])
    ])
    mst_geometries.append(line)

gdf_mst_routes = gpd.GeoDataFrame(route_mst_df, geometry=mst_geometries, crs='EPSG:4326')

# TSP ë…¸ì„  LineString
if tsp_route_df is not None:
    tsp_geometries = []
    for _, route in tsp_route_df.iterrows():
        start = cluster_summary[cluster_summary['cluster_id'] == route['from_cluster']].iloc[0]
        end = cluster_summary[cluster_summary['cluster_id'] == route['to_cluster']].iloc[0]
        line = LineString([
            (start['center_lon'], start['center_lat']),
            (end['center_lon'], end['center_lat'])
        ])
        tsp_geometries.append(line)

    gdf_tsp_routes = gpd.GeoDataFrame(tsp_route_df, geometry=tsp_geometries, crs='EPSG:4326')

# GeoJSON ì €ì¥
gdf_clusters.to_file(f"{output_path}ulsan_clusters.geojson", driver='GeoJSON', encoding='utf-8')
print(f"âœ“ {output_path}ulsan_clusters.geojson")

gdf_mst_routes.to_file(f"{output_path}ulsan_mst_routes.geojson", driver='GeoJSON', encoding='utf-8')
print(f"âœ“ {output_path}ulsan_mst_routes.geojson")

if tsp_route_df is not None:
    gdf_tsp_routes.to_file(f"{output_path}ulsan_tsp_route.geojson", driver='GeoJSON', encoding='utf-8')
    print(f"âœ“ {output_path}ulsan_tsp_route.geojson")

# ê°œë³„ ê´€ê´‘ì§€ í¬ì¸íŠ¸ (êµ°ì§‘ ì •ë³´ í¬í•¨)
gdf_points = gpd.GeoDataFrame(
    ulsan_data1,
    geometry=[Point(row['LONGITUDE'], row['LATITUDE']) for _, row in ulsan_data1.iterrows()],
    crs='EPSG:4326'
)
gdf_points.to_file(f"{output_path}ulsan_tour_spots.geojson", driver='GeoJSON', encoding='utf-8')
print(f"âœ“ {output_path}ulsan_tour_spots.geojson")

print("\n" + "=" * 50)
print("ë¶„ì„ ì™„ë£Œ!")
print("=" * 50)
print("\nğŸ“Š QGIS ì‹œê°í™” ê°€ì´ë“œ:")
print("1. ulsan_clusters.geojson - êµ°ì§‘ ì¤‘ì‹¬ì  (í¬ê¸°: total_usage)")
print("2. ulsan_mst_routes.geojson - MST ê¸°ë°˜ ì „ì²´ ë„¤íŠ¸ì›Œí¬")
print("3. ulsan_tsp_route.geojson - ìƒìœ„ êµ°ì§‘ ìˆœí™˜ ë…¸ì„ ")
print("4. ulsan_tour_spots.geojson - ê°œë³„ ê´€ê´‘ì§€ (ìƒ‰ìƒ: cluster)")






# =====================================
# í´ëŸ¬ìŠ¤í„° 2 íŠ¹ì • ê´€ê´‘ì§€ë§Œ í•„í„°ë§
# =====================================

# í•„í„°ë§í•  ê´€ê´‘ì§€ ëª©ë¡
target_spots = [
    'ê°„ì ˆê³¶',
    'ëª…ì„ êµ',
    'ìš¸ì‚°ì˜¹ê¸°ë°•ë¬¼ê´€',
    'ìš¸ì‚°í•´ì–‘ë°•ë¬¼ê´€',
    'ìš¸ì£¼ë¯¼ì†ë°•ë¬¼ê´€',
    'ì™¸ê³ ì‚° ì˜¹ê¸°ë§ˆì„',
    'ì§„í•˜í•´ìˆ˜ìš•ì¥'
]

# ë°ì´í„° ë¡œë“œ (ì´ë¯¸ êµ°ì§‘ë¶„ì„ì´ ì™„ë£Œëœ íŒŒì¼)
# ulsan_data1ì´ ë©”ëª¨ë¦¬ì— ìˆë‹¤ë©´ ë°”ë¡œ ì‚¬ìš©, ì—†ë‹¤ë©´ CSVì—ì„œ ë¡œë“œ
try:
    df = ulsan_data1.copy()
except NameError:
    # CSV íŒŒì¼ì—ì„œ ë¡œë“œ
    df = pd.read_csv("C:/Users/data/ulsan/ulsan_tour_clusters.csv", encoding='utf-8-sig')

# ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ í™•ì¸ (ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
# ê°€ëŠ¥í•œ ì»¬ëŸ¼ëª…: 'NAME', 'TOUR_NAME', 'SPOT_NAME' ë“±
# ì•„ë˜ ì½”ë“œì—ì„œ 'NAME'ì„ ì‹¤ì œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”
name_column = 'TRANM_x'  # â† ì‹¤ì œ ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ìœ¼ë¡œ ë³€ê²½

# ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
if name_column not in df.columns:
    print(f"âš ï¸  '{name_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
    print("\nì•„ë˜ ì½”ë“œì˜ 'name_column' ë³€ìˆ˜ë¥¼ ì‹¤ì œ ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
else:
    # í´ëŸ¬ìŠ¤í„° 2 í•„í„°ë§
    cluster2_data = df[df['cluster'] == 2].copy()

    print(f"í´ëŸ¬ìŠ¤í„° 2 ì „ì²´ ê´€ê´‘ì§€ ìˆ˜: {len(cluster2_data)}")
    print(f"\ní´ëŸ¬ìŠ¤í„° 2 ì „ì²´ ê´€ê´‘ì§€ ëª©ë¡:")
    print(cluster2_data[name_column].tolist())

    # íŠ¹ì • ê´€ê´‘ì§€ë§Œ í•„í„°ë§
    filtered_data = cluster2_data[cluster2_data[name_column].isin(target_spots)].copy()

    print(f"\ní•„í„°ë§ëœ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data)}")
    print(f"í•„í„°ë§ëœ ê´€ê´‘ì§€:")
    print(filtered_data[name_column].tolist())

    # ëˆ„ë½ëœ ê´€ê´‘ì§€ í™•ì¸
    found_spots = filtered_data[name_column].tolist()
    missing_spots = [spot for spot in target_spots if spot not in found_spots]

    if missing_spots:
        print(f"\nâš ï¸  í´ëŸ¬ìŠ¤í„° 2ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê´€ê´‘ì§€:")
        for spot in missing_spots:
            print(f"  - {spot}")
        print("\nì „ì²´ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ ì¤‘...")

        # ì „ì²´ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
        for spot in missing_spots:
            matches = df[df[name_column].str.contains(spot, na=False)]
            if len(matches) > 0:
                print(f"\n'{spot}' ê²€ìƒ‰ ê²°ê³¼:")
                print(matches[[name_column, 'cluster', 'LATITUDE', 'LONGITUDE']].to_string(index=False))

    # GeoDataFrame ìƒì„±
    if len(filtered_data) > 0:
        gdf = gpd.GeoDataFrame(
            filtered_data,
            geometry=[Point(row['LONGITUDE'], row['LATITUDE']) for _, row in filtered_data.iterrows()],
            crs='EPSG:4326'
        )

        # GeoJSON ì €ì¥
        output_path = "C:/data/ulsan/cluster2_selected_spots.geojson"
        gdf.to_file(output_path, driver='GeoJSON', encoding='utf-8')

        print(f"\nâœ… GeoJSON íŒŒì¼ ìƒì„± ì™„ë£Œ:")
        print(f"   {output_path}")
        print(f"\ní¬í•¨ëœ ê´€ê´‘ì§€: {len(filtered_data)}ê°œ")

        # í†µê³„ ì •ë³´ ì¶œë ¥
        print("\n[ì„ íƒëœ ê´€ê´‘ì§€ í†µê³„]")
        print(f"í‰ê·  ì´ìš©ê±´ìˆ˜: {filtered_data['COUNT'].mean():.0f}")
        print(f"ì´ ì´ìš©ê±´ìˆ˜: {filtered_data['COUNT'].sum():.0f}")

    else:
        print("\nâŒ í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# =====================================
# ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (ê´€ê´‘ì§€ëª…ì´ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ê²½ìš°)
# =====================================
print("\n" + "=" * 50)
print("ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ ì‹œë„")
print("=" * 50)

# í´ëŸ¬ìŠ¤í„° 2 ë°ì´í„°
cluster2_data = df[df['cluster'] == 2].copy()

# ë¶€ë¶„ ë¬¸ìì—´ë¡œ ë§¤ì¹­
filtered_data_partial = cluster2_data[
    cluster2_data[name_column].str.contains('|'.join(target_spots), na=False, case=False)
].copy()

print(f"\në¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ ì°¾ì€ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data_partial)}")

if len(filtered_data_partial) > 0:
    print("\nì°¾ì€ ê´€ê´‘ì§€:")
    print(filtered_data_partial[name_column].tolist())

    # GeoDataFrame ìƒì„±
    gdf_partial = gpd.GeoDataFrame(
        filtered_data_partial,
        geometry=[Point(row['LONGITUDE'], row['LATITUDE']) for _, row in filtered_data_partial.iterrows()],
        crs='EPSG:4326'
    )

    # GeoJSON ì €ì¥
    output_path_partial = "C:/Users/data/ulsan/cluster2_selected_spots_partial.geojson"
    gdf_partial.to_file(output_path_partial, driver='GeoJSON', encoding='utf-8')

    print(f"\nâœ… GeoJSON íŒŒì¼ ìƒì„± ì™„ë£Œ (ë¶€ë¶„ ë§¤ì¹­):")
    print(f"   {output_path_partial}")



    # =====================================
    # í´ëŸ¬ìŠ¤í„° 3 íŠ¹ì • ê´€ê´‘ì§€ë§Œ í•„í„°ë§
    # =====================================

    # í•„í„°ë§í•  ê´€ê´‘ì§€ ëª©ë¡
    target_spots = [
        'ì‘ìˆ˜ì²œ',
        'ììˆ˜ì •ë™êµ´ë‚˜ë¼',
        'ì‹ ë¶ˆì‚°',
        'ìš¸ì£¼ ì–¸ì–‘ìì„±',
        'íŒŒë˜ì†Œí­í¬',
        'í™ë¥˜í­í¬'
    ]

    # ë°ì´í„° ë¡œë“œ (ì´ë¯¸ êµ°ì§‘ë¶„ì„ì´ ì™„ë£Œëœ íŒŒì¼)
    try:
        df = ulsan_data1.copy()
    except NameError:
        # CSV íŒŒì¼ì—ì„œ ë¡œë“œ
        df = pd.read_csv("C:/Users/data/ulsan/ulsan_tour_clusters.csv", encoding='utf-8-sig')

    # ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ (ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •)
    name_column = 'TRANM_x'  # â† ì‹¤ì œ ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ìœ¼ë¡œ ë³€ê²½

    # ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    if name_column not in df.columns:
        print(f"âš ï¸  '{name_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
        print("\nì•„ë˜ ì½”ë“œì˜ 'name_column' ë³€ìˆ˜ë¥¼ ì‹¤ì œ ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
    else:
        # í´ëŸ¬ìŠ¤í„° 3 í•„í„°ë§
        cluster3_data = df[df['cluster'] == 3].copy()

        print("=" * 50)
        print(f"í´ëŸ¬ìŠ¤í„° 3 ë¶„ì„")
        print("=" * 50)
        print(f"í´ëŸ¬ìŠ¤í„° 3 ì „ì²´ ê´€ê´‘ì§€ ìˆ˜: {len(cluster3_data)}")
        print(f"\ní´ëŸ¬ìŠ¤í„° 3 ì „ì²´ ê´€ê´‘ì§€ ëª©ë¡:")
        for name in cluster3_data[name_column].tolist():
            print(f"  - {name}")

        # íŠ¹ì • ê´€ê´‘ì§€ë§Œ í•„í„°ë§ (ì •í™•í•œ ë§¤ì¹­)
        filtered_data = cluster3_data[cluster3_data[name_column].isin(target_spots)].copy()

        print(f"\n{'=' * 50}")
        print(f"ì •í™•í•œ ì´ë¦„ ë§¤ì¹­ ê²°ê³¼")
        print(f"{'=' * 50}")
        print(f"í•„í„°ë§ëœ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data)}")

        if len(filtered_data) > 0:
            print(f"\nâœ… ì°¾ì€ ê´€ê´‘ì§€:")
            for name in filtered_data[name_column].tolist():
                print(f"  - {name}")

        # ëˆ„ë½ëœ ê´€ê´‘ì§€ í™•ì¸
        found_spots = filtered_data[name_column].tolist()
        missing_spots = [spot for spot in target_spots if spot not in found_spots]

        if missing_spots:
            print(f"\nâš ï¸  ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê´€ê´‘ì§€:")
            for spot in missing_spots:
                print(f"  - {spot}")

        # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
        print(f"\n{'=' * 50}")
        print(f"ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ ì‹œë„")
        print(f"{'=' * 50}")

        filtered_data_partial = pd.DataFrame()

        for target in target_spots:
            # ê° í‚¤ì›Œë“œë³„ë¡œ ë¶€ë¶„ ë§¤ì¹­
            matches = cluster3_data[
                cluster3_data[name_column].str.contains(target, na=False, case=False)
            ]

            if len(matches) > 0:
                print(f"\n'{target}' ê²€ìƒ‰ ê²°ê³¼:")
                for name in matches[name_column].tolist():
                    print(f"  â†’ {name}")
                filtered_data_partial = pd.concat([filtered_data_partial, matches])

        # ì¤‘ë³µ ì œê±°
        filtered_data_partial = filtered_data_partial.drop_duplicates()

        print(f"\n{'=' * 50}")
        print(f"ìµœì¢… ê²°ê³¼")
        print(f"{'=' * 50}")
        print(f"ë¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ ì°¾ì€ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data_partial)}")

        if len(filtered_data_partial) > 0:
            print("\nìµœì¢… ì„ íƒëœ ê´€ê´‘ì§€:")
            for idx, row in filtered_data_partial.iterrows():
                print(f"  - {row[name_column]} (ì´ìš©ê±´ìˆ˜: {row['COUNT']:,})")

            # GeoDataFrame ìƒì„±
            gdf = gpd.GeoDataFrame(
                filtered_data_partial,
                geometry=[Point(row['LONGITUDE'], row['LATITUDE']) for _, row in filtered_data_partial.iterrows()],
                crs='EPSG:4326'
            )

            # GeoJSON ì €ì¥
            output_path = "C:/Users/data/ulsan/cluster3_selected_spots.geojson"
            gdf.to_file(output_path, driver='GeoJSON', encoding='utf-8')

            print(f"\nâœ… GeoJSON íŒŒì¼ ìƒì„± ì™„ë£Œ:")
            print(f"   {output_path}")

            # í†µê³„ ì •ë³´
            print(f"\n[ì„ íƒëœ ê´€ê´‘ì§€ í†µê³„]")
            print(f"ì´ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data_partial)}")
            print(f"í‰ê·  ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].mean():.0f}")
            print(f"ì´ ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].sum():.0f}")
            print(f"ìµœëŒ€ ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].max():.0f}")
            print(f"ìµœì†Œ ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].min():.0f}")

        else:
            print("\nâŒ í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("\nì „ì²´ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ ì¤‘...")

            # ì „ì²´ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
            for spot in target_spots:
                print(f"\n'{spot}' ì „ì²´ ê²€ìƒ‰:")
                matches = df[df[name_column].str.contains(spot, na=False, case=False)]
                if len(matches) > 0:
                    for _, row in matches.iterrows():
                        print(
                            f"  â†’ {row[name_column]} (í´ëŸ¬ìŠ¤í„°: {row['cluster']}, ìœ„ë„: {row['LATITUDE']}, ê²½ë„: {row['LONGITUDE']})")
                else:
                    print(f"  â†’ ì°¾ì„ ìˆ˜ ì—†ìŒ")

    # =====================================
    # í´ëŸ¬ìŠ¤í„° 3 ì „ì²´ ê´€ê´‘ì§€ ëª©ë¡ CSV ì €ì¥
    # =====================================
    try:
        cluster3_full = df[df['cluster'] == 3].copy()
        output_csv = "C:/Users/data/ulsan/cluster3_all_spots.csv"
        cluster3_full.to_csv(output_csv, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“„ í´ëŸ¬ìŠ¤í„° 3 ì „ì²´ ëª©ë¡ CSV ì €ì¥:")
        print(f"   {output_csv}")
    except:
        pass

# =====================================
# í´ëŸ¬ìŠ¤í„° 5 íŠ¹ì • ê´€ê´‘ì§€ë§Œ í•„í„°ë§
# =====================================

# í•„í„°ë§í•  ê´€ê´‘ì§€ ëª©ë¡
target_spots = [
    'ìš¸ì‚°ëŒ€ê³¡ë°•ë¬¼ê´€',
    'ìš¸ì‚°ì•”ê°í™”ë°•ë¬¼ê´€',
    'ì¶©ë ¬ê³µë°•ì œìƒìœ ì ì§€',
    'ìš¸ì£¼ ëŒ€ê³¡ë¦¬ ë°˜êµ¬ëŒ€ì•”ê°í™”',
    'ìš¸ì£¼ ì²œì „ë¦¬ ê°ì„',
    'ìš¸ì‚°ì–´ë¦°ì´ì²œë¬¸ëŒ€'
]

# ë°ì´í„° ë¡œë“œ (ì´ë¯¸ êµ°ì§‘ë¶„ì„ì´ ì™„ë£Œëœ íŒŒì¼)
try:
    df = ulsan_data1.copy()
except NameError:
    # CSV íŒŒì¼ì—ì„œ ë¡œë“œ
    df = pd.read_csv("C:/Users/data/ulsan/ulsan_tour_clusters.csv", encoding='utf-8-sig')

# ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ (ì‹¤ì œ ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •)
name_column = 'TRANM_x'  # â† ì‹¤ì œ ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ìœ¼ë¡œ ë³€ê²½

# ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
if name_column not in df.columns:
    print(f"âš ï¸  '{name_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
    print("\nì•„ë˜ ì½”ë“œì˜ 'name_column' ë³€ìˆ˜ë¥¼ ì‹¤ì œ ê´€ê´‘ì§€ëª… ì»¬ëŸ¼ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
else:
    # í´ëŸ¬ìŠ¤í„° 5 í•„í„°ë§
    cluster5_data = df[df['cluster'] == 5].copy()

    print("=" * 50)
    print(f"í´ëŸ¬ìŠ¤í„° 5 ë¶„ì„")
    print("=" * 50)
    print(f"í´ëŸ¬ìŠ¤í„° 5 ì „ì²´ ê´€ê´‘ì§€ ìˆ˜: {len(cluster5_data)}")
    print(f"\ní´ëŸ¬ìŠ¤í„° 5 ì „ì²´ ê´€ê´‘ì§€ ëª©ë¡:")
    for name in cluster5_data[name_column].tolist():
        print(f"  - {name}")

    # íŠ¹ì • ê´€ê´‘ì§€ë§Œ í•„í„°ë§ (ì •í™•í•œ ë§¤ì¹­)
    filtered_data = cluster5_data[cluster5_data[name_column].isin(target_spots)].copy()

    print(f"\n{'=' * 50}")
    print(f"ì •í™•í•œ ì´ë¦„ ë§¤ì¹­ ê²°ê³¼")
    print(f"{'=' * 50}")
    print(f"í•„í„°ë§ëœ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data)}")

    if len(filtered_data) > 0:
        print(f"\nâœ… ì°¾ì€ ê´€ê´‘ì§€:")
        for name in filtered_data[name_column].tolist():
            print(f"  - {name}")

    # ëˆ„ë½ëœ ê´€ê´‘ì§€ í™•ì¸
    found_spots = filtered_data[name_column].tolist()
    missing_spots = [spot for spot in target_spots if spot not in found_spots]

    if missing_spots:
        print(f"\nâš ï¸  ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê´€ê´‘ì§€:")
        for spot in missing_spots:
            print(f"  - {spot}")

    # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
    print(f"\n{'=' * 50}")
    print(f"ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ ì‹œë„")
    print(f"{'=' * 50}")

    filtered_data_partial = pd.DataFrame()

    # ê° í‚¤ì›Œë“œë³„ ê²€ìƒ‰ (ë” ì •êµí•œ ë§¤ì¹­)
    search_keywords = {
        'ìš¸ì‚°ëŒ€ê³¡ë°•ë¬¼ê´€': ['ìš¸ì‚°ëŒ€ê³¡ë°•ë¬¼ê´€'],
        'ìš¸ì‚°ì•”ê°í™”ë°•ë¬¼ê´€': ['ìš¸ì‚°ì•”ê°í™”ë°•ë¬¼ê´€'],
        'ì¶©ë ¬ê³µë°•ì œìƒìœ ì ì§€': ['ì¶©ë ¬ê³µë°•ì œìƒìœ ì ì§€'],
        'ìš¸ì£¼ ëŒ€ê³¡ë¦¬ ë°˜êµ¬ëŒ€ì•”ê°í™”': ['ìš¸ì£¼ ëŒ€ê³¡ë¦¬ ë°˜êµ¬ëŒ€ì•”ê°í™”'],
        'ìš¸ì£¼ ì²œì „ë¦¬ ê°ì„': ['ìš¸ì£¼ ì²œì „ë¦¬ ê°ì„'],
        'ìš¸ì‚°ì–´ë¦°ì´ì²œë¬¸ëŒ€': ['ìš¸ì‚°ì–´ë¦°ì´ì²œë¬¸ëŒ€']
    }

    for target, keywords in search_keywords.items():
        found = False
        for keyword in keywords:
            matches = cluster5_data[
                cluster5_data[name_column].str.contains(keyword, na=False, case=False)
            ]

            if len(matches) > 0 and not found:
                print(f"\n'{target}' ê²€ìƒ‰ ê²°ê³¼ (í‚¤ì›Œë“œ: '{keyword}'):")
                for name in matches[name_column].tolist():
                    print(f"  â†’ {name}")
                filtered_data_partial = pd.concat([filtered_data_partial, matches])
                found = True
                break

        if not found:
            print(f"\n'{target}' â†’ ì°¾ì„ ìˆ˜ ì—†ìŒ")

    # ì¤‘ë³µ ì œê±°
    filtered_data_partial = filtered_data_partial.drop_duplicates()

    print(f"\n{'=' * 50}")
    print(f"ìµœì¢… ê²°ê³¼")
    print(f"{'=' * 50}")
    print(f"ë¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ ì°¾ì€ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data_partial)}")

    if len(filtered_data_partial) > 0:
        print("\nìµœì¢… ì„ íƒëœ ê´€ê´‘ì§€:")
        for idx, row in filtered_data_partial.iterrows():
            print(f"  - {row[name_column]} (ì´ìš©ê±´ìˆ˜: {row['COUNT']:,})")

        # GeoDataFrame ìƒì„±
        gdf = gpd.GeoDataFrame(
            filtered_data_partial,
            geometry=[Point(row['LONGITUDE'], row['LATITUDE']) for _, row in filtered_data_partial.iterrows()],
            crs='EPSG:4326'
        )

        # GeoJSON ì €ì¥
        output_path = "C:/Users/data/ulsan/cluster5_selected_spots.geojson"
        gdf.to_file(output_path, driver='GeoJSON', encoding='utf-8')

        print(f"\nâœ… GeoJSON íŒŒì¼ ìƒì„± ì™„ë£Œ:")
        print(f"   {output_path}")

        # í†µê³„ ì •ë³´
        print(f"\n[ì„ íƒëœ ê´€ê´‘ì§€ í†µê³„]")
        print(f"ì´ ê´€ê´‘ì§€ ìˆ˜: {len(filtered_data_partial)}")
        print(f"í‰ê·  ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].mean():.0f}")
        print(f"ì´ ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].sum():.0f}")
        print(f"ìµœëŒ€ ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].max():.0f}")
        print(f"ìµœì†Œ ì´ìš©ê±´ìˆ˜: {filtered_data_partial['COUNT'].min():.0f}")

        # ê´€ê´‘ì§€ë³„ ìƒì„¸ ì •ë³´
        print(f"\n[ê´€ê´‘ì§€ë³„ ìƒì„¸ ì •ë³´]")
        for idx, row in filtered_data_partial.iterrows():
            print(f"\n{row[name_column]}")
            print(f"  - ìœ„ë„: {row['LATITUDE']:.6f}")
            print(f"  - ê²½ë„: {row['LONGITUDE']:.6f}")
            print(f"  - ì´ìš©ê±´ìˆ˜: {row['COUNT']:,}")
            print(f"  - ì •ê·œí™”ê°’: {row['Value']:.3f}")

    else:
        print("\nâŒ í´ëŸ¬ìŠ¤í„° 5ì—ì„œ í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("\nì „ì²´ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ ì¤‘...")

        # ì „ì²´ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
        for spot in target_spots:
            print(f"\n'{spot}' ì „ì²´ ê²€ìƒ‰:")
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µë°± ê¸°ì¤€ ë¶„ë¦¬)
            keywords = spot.split()
            found_any = False

            for keyword in keywords:
                if len(keyword) > 1:  # 1ê¸€ì í‚¤ì›Œë“œ ì œì™¸
                    matches = df[df[name_column].str.contains(keyword, na=False, case=False)]
                    if len(matches) > 0:
                        found_any = True
                        for _, row in matches.iterrows():
                            print(f"  â†’ {row[name_column]} (í´ëŸ¬ìŠ¤í„°: {row['cluster']}, ì´ìš©ê±´ìˆ˜: {row['COUNT']:,})")

            if not found_any:
                print(f"  â†’ ì „ì²´ ë°ì´í„°ì—ì„œë„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

# =====================================
# í´ëŸ¬ìŠ¤í„° 5 ì „ì²´ ê´€ê´‘ì§€ ëª©ë¡ CSV ì €ì¥
# =====================================
try:
    cluster5_full = df[df['cluster'] == 5].copy()
    output_csv = "C:/Users/Desktop/data/ulsan/cluster5_all_spots.csv"
    cluster5_full.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“„ í´ëŸ¬ìŠ¤í„° 5 ì „ì²´ ëª©ë¡ CSV ì €ì¥:")
    print(f"   {output_csv}")
    print(f"   (ì´ {len(cluster5_full)}ê°œ ê´€ê´‘ì§€)")
except:
    pass

print("\n" + "=" * 50)
print("ì‘ì—… ì™„ë£Œ!")
print("=" * 50)